---
title: "Statistics and Predictive Analytics, Lab 2"
author: "Swati Singh"
output:
  html_document: default
  pdf_document: default
---

Welcome to Lab 2!  

- The lab is due through Canvas by 6 PM before the next class meeting.
- Make sure to write your name in the yaml header above.  
- Take care that the output type in the header remains "html_notebook." (If you accidentally change it, by clicking "Knit to HTML" for example," simply change the output back to "html_notebook" and save.  This should restore the "Preview" option for you.) 
- Before compiling, click the "Run" button in the upper left of the RStudio toolbar, and select "run all."  This will ensure that your code chunks have run and will be visible to us in the compiled HTML document that you submit for the grade.
- Click the "Preview" button on the toolbar to compile your notebook into HTML.  This resulting HTML document is what you will submit through the Lab 1 assignment.
- The HTML answer key for this notebook is available for you to check, if you want to do so.  

### Introduction

The purpose of the lab is to give you practice with the skills we have covered so far in the course, from EDA to linear regression:  

- Create exploratory plots and think about data modeling.
- Fit and interpret simple and multivariable linear models.
- Interpret effect sizes.
- Calculate error metrics and compare models.
- Calculate quantities of interest.

You will be working with the Boston housing dataset that is available in the MASS package.  This dataset records tract level statistics on housing in the Boston area in the 1970s.  The outcome variable is `medv`:  Median Home Value (expressed in $1000s).  

Our goal in this analysis, specifically, will be to find out whether air pollution, encoded in the variable `nox`, impacts home values.  Are increases in nitrogen oxide associated with lower home values? Or, to put the question the other way around, is clean air associated with more expensive homes?  Data analysis can be focused on prediction or description, also known as inference.  Our goal here is the second.


```{r message=FALSE, warning=FALSE}
library(tidyverse)
# install.packages("MASS") # install the MASS package (if you don't already have it)
library(MASS)
library(knitr)
library(caret)
library(arm)

data(Boston)

?Boston # Look at the data dictionary 

b <- Boston # Rename to avoid more typing than necessary
```


A more complete data dictionary is available in the original Harrison & Rubinfeld paper that used the dataset, starting on page 96: https://www.law.berkeley.edu/files/Hedonic.PDF.


### Data exploration and modeling

The `summary()` function allows you to quickly assess the distributions of the variables, including which ones are continuous and which are binary, and whether there are any missing observations.  

```{r}
summary(b)  # Get a summary of each variable.  
```

For example, we can see that "chas" is binary: it is an indicator for whether the tract (not the home) bounds the Charles River.  Another handy tool is `glimpse()` from dplyr, which will give you a sense of the structure of the data and of the observations themselves.

```{r}
glimpse(b)   # Look at the structure of the dataset
```

We can review the univariate distributions of the predictors in the dataset using histograms like this:

```{r}

for(i in 1:length(names(b))){
  hist(b[,i], main = names(b)[i])
}
```

```{r}
skimr::skim(b)
```

What are we looking for exactly?  We want to be familiar with the distributions of the predictors before we jump into regression modeling to inform choices about how to clean and encode variables:  are there outliers? nonsensical values? should a variable be encoded as a factor or left numeric? would it make sense to do any other variable transformations such as turning a continuous variable into a binary or categorical variable?

```{r}
#Chas can be converted to factor
b$chas<-as.factor(b$chas)

```
For example, the rad variable (an index of the highway accessibility) has a strange distribution.  Here is the scatterplot of rad against medv:

```{r}
ggplot(b, aes(rad, medv)) +
  geom_point() +
  labs(title = "Medv ~ rad")

table(b$rad) # Here is the distribution of rad.
```

Is rad best left as numeric or should it be treated as a factor variable or should we discretize it (< 9 and > 9)?  These are the sorts of questions we need to think about at the data exploration/data modeling phase of an analytic project.

Rad = 24 is a weird value.  Nevertheless, numeric differences in rad are meaningful, in that higher levels represent less accessibility (and lower represent greater).  Harrison and Rubinfeld entered Rad into the model as a logged variable. It looks like this would actually make the fit worse, though the scatterplot looks better.  Turning rad into a factor variable fits a separate model for each level of rad.  The concern in doing that is we may be overfitting.  I would leave rad as numeric.

**Question 1**: Calculate and report average `nox` in this dataset with 95% CIs.  

```{r}
# Write your code here
b %>%
  summarize(Avg_Nox= mean(nox),
            SEM=sd(nox)/sqrt(n()),
            Lower=Avg_Nox-1.96*SEM,
            Upper=Avg_Nox+1.96*SEM)
```

**Question 2**: Plot the relationship between `nox` and `medv` (i.e., make a scatterplot with a least squares line).  Make sure to title the plot and label the axes. 

```{r}
# Write your code here
ggplot(b,aes(nox,medv))+
  geom_point()+
  stat_smooth(method = "lm")+
  labs(title="Relationship between NOX and MEDV")
```

## Fitting and interpreting models

*Question 3*:  Fit a simple linear regression with medv as the outcome variable and nox as the predictor. Report the 95% confidence interval for $\hat\beta$ (the coefficient for nox) and comment on whether its difference from 0 might be due to chance. 


```{r}
# Your code goes here
model_nox<-lm(medv~nox,b)
summary(model_nox)
```
```{r}
SEM<-3.196
Est<--33.916
Lower<-Est-1.96*SEM
Lower
Upper<-Est+1.96*SEM
Upper
```
>The range is between -40.18 to -27.65. Based on the p-value we can determine that nox has a statistically significant effect on the medv value.

**Question 4**: Fit a linear model of `medv` with all the predictors (we will call this the "full model") and interpret the coefficient for `nox`.  By "interpret" I mean you should explain what the coefficient says about the relationship between  `nox` and `medv`.  Additionally, report a 95% CI for the `nox` coefficient.  

```{r}
# Write your code here. 
model_all<-lm(medv~.,b)
summary(model_all)
```
```{r}
SEM<-3.82
Est<--17.77
Lower<-Est-1.96*SEM
print(paste("Lower interval: ",Lower))
Upper<-Est+1.96*SEM
print(paste("Upper Interval: ",Upper))

```
>There is negative relationship between nox levels and median value of homes. This means that for every unit increase in nox level, medv value decreases by 17.7 units. However the effect size has decreased from 33.9 units to 17.7 units from that of a simple model to a full model. The decrease in effect size can be explained by the effect of other variables in the model. Nox still has a significant effect on the medv value based on p-value.


### Effect sizes

**Question 5**: Which of the predictors in the full model has the largest effect size? 

Beware of differently scaled variables that can produce misleading effect sizes! You should rescale the variables to answer this question, using either `rescale` or `standardize` from the arm package, or, if you want to use caret, include the `preProcess = c("center", "scale")` argument in the train() function.  Remember that by "effect size" we mean the variable whose coefficient has the largest absolute scaled value. 

```{r}
# Your code goes here
#Standardize
#Combining the scale and center transforms will standardize the data. Attributes will have a mean value of 0 and #a standard deviation of 1.
full_scaled_model<- train(medv~.,b,preProcess = c("center","scale"),
      method = "lm")
summary(full_scaled_model)$coefficients
```

>lstat has the largest effect size based on absolute value

### Error metrics and model comparison

**Question 6**:  Report the $R^2$ and RMSE of the full model.  You can report $R^2$ from the model summary, of course, but see if you can write a function to calculate $R^2$ (refer to the formulas in the lecture slides).

```{r}
# your code goes here
print(paste("R-Squard from model summary",summary(model_all)$r.squared))

print(paste("Adjusted R Squared from model summary", summary(model_all)$adj.r.squared))

rmse<-function(model){
  sqrt(sum(model$residuals^2)/model$df)
}
  
print(paste("RMSE from function",round(rmse(model_all),2)))

r2<-function(pred,actual){
  SSE<-sum((pred-actual)^2)
  SSTO<-sum((actual-mean(actual))^2)
  return(1-(SSE/SSTO)) 
}

print(paste("R Squared from function",r2(fitted(model_all),b$medv)))
```


**Question 7**: Fit a KNN model using the variables the full model.  Does the KNN model fit the data better than your linear model?  Explain your reasoning. Sometimes, for example, if a non-parametric model fits the sample data better, it can be because the parametric assumptions of a linear model are being violated. (It is also possible that the non-parametric model is overfitting the sample data.  More on that in a future class.)

```{r}
# Your code goes here
set.seed(1234)
full_knn_model<- train(medv~.,b,preProcess = c("center","scale"),
      method = "knn")
r2(fitted(full_knn_model),b$medv)
plot(b$medv,fitted(full_knn_model))
abline(lm(b$medv~fitted(full_knn_model)))

plot(b$medv,fitted(full_scaled_model))  
abline(lm(b$medv~fitted(full_scaled_model)))

print(paste("R squared of KNN model is",r2(fitted(full_knn_model),b$medv)))
print(paste("R squared of Linear model is",r2(fitted(full_scaled_model),b$medv)))

```

>The KNN model is fitting the data better.Usually, larger the r squared, better the model fits your data. The r sqaured of KNN is larger than that of the linear model. Also when we look at the scatter plot of the fitted values vs actual values, for KNN model, the observations are closer to the regression line. Closer the observations are to the regression line,better the model explains the variance in the data.

### Statistical communication

**Question 8**: Our original question was:  "is clean air associated with more expensive homes?"  Write a paragraph in which you address that question, including any thoughts you have about the shortcoming of the analysis or your level of confidence in the results. 

>Lower NOX levels can result in cleaner air. According to the full model summary, NOX has a sinificant effect on the median value of homes. It also has a considerably large effect size as compared to other features. We can say that clean air can be associated with more expensive homes.

In class I made the point that regression model results generally should be translated into quantities of interest so that non-statistical audiences can better understand them.  

**Question 9 (optional)**: Using the full model calculate the value of a typical home at a low level of `nox` and at a high level of `nox`.  It is up to you to define what counts as "low" or "high."  Your goal should be to complete the following sentence:  "When `nox` increases from a low level (defined by ...) to a high level (defined by ...) we observe an increase/decrease in home values of, on average, ... dollars."

```{r}
# Your code goes here
hist(b$nox)
cutoff<-median(b$nox)
print(paste("Cutoff level: ",cutoff))
b$predict<-predict(model_all,b)
```

The data is right skewed. hence to will use median value as measure of central tendency.

```{r}
Boston$nox<-ifelse(Boston$nox<cutoff,"low","high")
Boston$nox<-as.factor(Boston$nox)

ggplot(Boston,aes(x=medv,color=nox))+
  geom_histogram(position = "identity")
```



```{r}

low_nox<-subset(b,nox<cutoff)
high_nox<-subset(b,nox>cutoff)
mean_low_nox<-median(low_nox$predict)

high_nox$diff<-high_nox$predict-mean_low_nox
print(paste("On an average, median value decreases by ",
            1000*abs(round(mean(high_nox$diff),0)),
            " dollars when nox level goes high from low"
            )
      )
print(paste("Average price of house:",
            1000*round(mean(b$medv),1),
            " dollars"
            )
      )

```

```{r}
median_low_nox=median(low_nox$nox)
print(median_low_nox)
median_high_nox=median(high_nox$nox)
print(median_high_nox)
low_nox_df<-data.frame(crim=median(b$crim),
                       zn=median(b$zn),
                       indus=median(b$indus),
                       chas="0",
                       nox=median_low_nox,
                       rm=median(b$rm),
                       age=median(b$age),
                       dis=median(b$dis),
                       rad=median(b$rad),
                       tax=median(b$tax),
                       ptratio=median(b$ptratio),
                       black=median(b$black),
                       lstat=median(b$lstat),
                       medv=median(b$medv))
high_nox_df<-data.frame(crim=median(b$crim),
                       zn=median(b$zn),
                       indus=median(b$indus),
                       chas="0",
                       nox=median_high_nox,
                       rm=median(b$rm),
                       age=median(b$age),
                       dis=median(b$dis),
                       rad=median(b$rad),
                       tax=median(b$tax),
                       ptratio=median(b$ptratio),
                       black=median(b$black),
                       lstat=median(b$lstat),
                       medv=median(b$medv))

print(paste("Keeping all other parameters constant and changing the nox levels, average price decreases by ",
      1000*round(predict(full_scaled_model,low_nox_df)-predict(full_scaled_model,high_nox_df),1)
            ))

```
>Looking at the distribution in the histogram, we can infer house with higher median value are primarliy in low nox level neighborhoods.
Keeping all parameters as it is and calculating the difference between predicted median values of houses and median house value in low nox neighborhood, we found a difference/decrease of 6000$ when nox increases from low to high level.
Keeping all parameters constant at median value, and increasing nox level from 0.448 to 0.647, we found a decrease of 3500$ in median house value. This is a significant change in median value considering the average median value of a house is 22500$.

**Question 10**:  Please score yourself on this lab using the answer key.  For each question, reflect on the difficulty of the question for you and whether you:

- Did not try (just copied the answer key or left it blank).
- Looked at the answer key to get a hint.
- Tried but got the wrong answer.
- Tried and got the right answer.

>Tried and got right answer. Took hint for formulate scenario, 'keeping all paratmeters constant for Q9'.

**Next step.**  After you've finished this lab, go take Quiz 2.  You have one try at the quiz, limited to 30 minutes.  There are five multiple choice and multiple answer questions focusing on the material from this lab.

